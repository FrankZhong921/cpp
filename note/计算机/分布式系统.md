







# 分布式理论

## CAP理论

- **一致性(Consistency)**：所有节点在同一时间的数据完全一致

  - `all nodes see the same data at the same time`

  - 对于一致性，可以分为从客户端和服务端两个不同的视角。

    - 从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。
    - 从服务端来看，则是更新如何分布到整个系统，以保证数据最终一致。

  - 对于一致性，可以分为强/弱/最终一致性三类

    - 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。

    - 弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。

    - 最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。

- **可用性(Availabiliay)**：服务在正常响应时间内一直可用

  - `Reads and writes always succeed`

- **分区容错性(Partition Tolerance)**：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。

  - `the system continues to operate despite arbitrary message loss or failure of part of the system`

### CAP的简单证明：

假设有两个节点 n1 和 n2 之间通过网络相连，每个节点内对应的持久化数据为 d1 和 d2．

n1 节点接受客户请求，更新数据d1，而此时 n1 和 n2之间的网络断开，n1无法将该更新通知给 n2，此时在一致性和可用性只能二选一：

1. 若选择可用性，即 n2节点应该响应旧数据给用户，不能保证一致性
2. 若选择一致性，即 n2节点等待网络恢复之后进行数据一致同步，再响应新数据给用户，此时响应取决与网络何时恢复，不能保证可用性．

### CAP的权衡

#### CAP是二选一的问题而不是三选二的问题

首先应该明确，分区容错性是必须保证的，若没有分区容错性，一旦单机崩溃就没有一致性和可用性之谈．

#### CAP的权衡没有定论，只能根据适用场景

- **CP without A**：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。对于涉及到钱财这样不能有一丝让步的场景，C必须保证。
- **AP wihtout C**：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。

### 举例

以注册中心来探讨一下 CAP 的实际应用。

- 什么是注册中心？

  注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。**节点向注册中心注册服务，用户订阅．**

  <img src="https://pic4.zhimg.com/v2-b0608ad8770af0a37cc0649d99ab13d7_r.jpg" style="zoom:100%;" />

  1. **ZooKeeper 保证的是 CP。** 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。
  2. **Eureka 保证的则是 AP。** Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么  Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper  那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka  保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。
  3. **Nacos 不仅支持 CP 也支持 AP。**



## BASE理论

BASE 理论是**对 CAP 中一致性 C 和可用性 A 权衡的结果**(AP方案的一个补充)，**即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。**其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。

- **Basically Available（基本可用）**：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这**绝不等价于系统不可用。**
  - 什么叫允许损失部分可用性：
    - **响应时间上的损失**: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
    - **系统功能上的损失**：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。
- **Soft-state（软状态）** ：允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- **Eventually Consistent（最终一致性）**:最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。即延迟`at the same time`

## 总结

**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**



# Two-phase Commit协议

[Two-phase Commit协议 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/129000919)

# 论文阅读

# MapReduce

## Programming Model

The computation takes a set of input key/value pairs, and produces a set of output key/value pairs. The user of the MapReduce library expresses the computation as two functions: Map and Reduce. 

- Map, written by the user, takes an input pair and produces a set of intermediate key/value pairs. 
- The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function. 
- The Reduce function, also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory.

## Example

Consider the problem of counting the number of currences of each word in a large collection of documents. The user would write code similar to the following pseudo-code:

```c++
map(String key, String values):
	// key : document name
	// values : document content
	for each word w in values:
		EmitIntermedia(w,"1");
reduce(String key, Iterator values):
	// key : a word
	// values : a list of counts
	int result = 0;
	for each v in values:
		result += ParseInt(v);
	Emit(AsString(result));
```

In addition, the user writes code to fill in a mapreduce specification object with the names of the input and output files, and optional tuning parameters. The user then invokes the MapReduce function, passing it the specification object. The user’s code is linked together with the MapReduce library (implemented in C++).

### Example implement

```c++
#include "mapreduce/mapreduce.h"
// User’s map function
class WordCounter : public Mapper {
	public:
		virtual void Map(const MapInput& input) {
		const string& text = input.value();
		const int n = text.size();
		for (int i = 0; i < n; ) {
		// Skip past leading whitespace
			while ((i < n) && isspace(text[i]))
				i++;
			// Find word end
			int start = i;
			while ((i < n) && !isspace(text[i]))
				i++;
            if (start < i)
				Emit(text.substr(start,i-start),"1");
		}
	}
};

REGISTER_MAPPER(WordCounter);
// User’s reduce function
class Adder : public Reducer {
		virtual void Reduce(ReduceInput* input) {
		// Iterate over all entries with the
		// same key and add the values
		int64 value = 0;
		while (!input->done()) {
			value += StringToInt(input->value());
			input->NextValue();
		}
		// Emit sum for input->key()
		Emit(IntToString(value));
	}
};
REGISTER_REDUCER(Adder);
int main(int argc, char** argv) {
	ParseCommandLineFlags(argc, argv);
	MapReduceSpecification spec;
	// Store list of input files into "spec"
	for (int i = 1; i < argc; i++) {
	MapReduceInput* input = spec.add_input();
		input->set_format("text");
    	input->set_filepattern(argv[i]);
		input->set_mapper_class("WordCounter");
	}
    // Specify the output files:
    // /gfs/test/freq-00000-of-00100
    // /gfs/test/freq-00001-of-00100
    // ...
    MapReduceOutput* out = spec.output();
    out->set_filebase("/gfs/test/freq");
    out->set_num_tasks(100);
    out->set_format("text");
    out->set_reducer_class("Adder");
    // Optional: do partial sums within map
    // tasks to save network bandwidth
    out->set_combiner_class("Adder");
    // Tuning parameters: use at most 2000
    // machines and 100 MB of memory per task
    spec.set_machines(2000);
    spec.set_map_megabytes(100);
    spec.set_reduce_megabytes(100);
    // Now run it
    MapReduceResult result;
    if (!MapReduce(spec, &result)) abort();
    // Done: ’result’ structure contains info
    // about counters, time taken, number of
    // machines used, etc.
    return 0;
}
```

## Implement

<img src="E:\cpp\note\note_picture\image-20210607001425646.png" alt="image-20210607001425646" style="zoom:80%;" />

Many different implementations of the MapReduce interface are possible. The right choice depends on the environment. For example, one implementation may be suitable for a small shared-memory machine, another for a large NUMA multi-processor, and yet another for an even larger collection of networked machines.

- MapReduce的接口实现应该依赖于环境，有的适合在小型共享内存的机器运行，有的适合大型NUMA多处理器，还有的适合更大的联网机器集群。
- **非统一内存访问（NUMA）**是一种用于多处理器的电脑内存体设计，内存访问时间取决于处理器的内存位置。 在NUMA下，处理器访问它自己的本地存储器的速度比非本地存储器（存储器的地方到另一个处理器之间共享的处理器或存储器）快一些。
  - NUMA通过提供分离的存储器给各个处理器，避免当多个处理器访问同一个存储器产生的性能损失来试图解决这个问题。对于涉及到分散的数据的应用（在服务器和类似于服务器的应用中很常见），NUMA可以通过一个共享的存储器提高性能至n倍,而n大约是处理器（或者分离的存储器）的个数。

When the user program calls the MapReduce function, the following sequence of actions occurs

1. MapReduce library 先将输入的文件集合分成M块，每块通常是16MB到64MB（可由用户设定），然后将程序拷贝到计算机集群中运行
2. 一个比较特殊的计算机节点——Master，将M个map任务和R个Reduce任务分配给空闲的机器（worker）。
3. 被分配到map任务的机器读取输入文件块(piece)，以键值对进行解析，然后将键值对传递给用户定义的Map函数，最终产生中间键值对缓存到内存中。
4. 被缓存的中间键值对周期地写入本地磁盘，并使用切分函数(partitioning function)分成R个区(region)。这些键值对在磁盘上的位置会传回到master节点上，用来转发给负责reduce worker。
5. 当master通知reduce worker这些位置时，reduce worker使用远程过程调用RPC读取map worker磁盘上的数据。当读取完所有键值对后，根据中间键进行排序，方便具有相同键的数据进行合并。（之所以需要进行排序，是因为通常有许多不同的键映射到相同的reduce任务。）如果数据量太大，还需要使用**外部排序**(extetnal sort)
6. reduce worker遍历已排序的中间键值对，对于每个唯一的键，将键和相对应的合并后的值的集合传递给Reduce函数，针对该键输出的结果被追加到一个最终输出文件。（有多少个唯一键就追加多少次，但最终一个reduce worker只产生一个输出文件）
7. 当所有map任务和reduce任务都完成时，master唤醒用户程序，MapReduce的调用返回结果。

当一切执行完毕后会产生R个输出文件，通常无需合并，因为这些文件通常会作为另一MapReduce的输入。

## Master Data Structure

Master维护几个数据结构，对于每个map和reduce任务，将存储它们的状态，包括: *idle* ,*in-progress*, *completed*和worker machine的身份。

Master主要是分配任务，并且作为map worker和reduce worker传输intermedia k/v pair的中心枢纽。

## Fault Tolerance

### worker failure

master周期地ping worker，如果一定时间内没有响应，则标记为failed.

1. 正在被被该worker完成的map任务或reduce任务被设置为idle状态，因此可以重新被分配。

2. 已完成的map任务因为中间键值对存储在磁盘上变得不可访问，因此需要被重新执行。

3. 已完成的reduce任务因为结果输出在一个全局可见的文件系统，因此不需要重新执行。

### Master failure

可以简单地让master节点周期性地设置有关master data structure检查点(check point)，如果Master task dies(这用的是task die)，程序可以从最新的检查点重新运行。不过考虑到通常只有一个master，通常是不会失败，如果master fail我们直接终止(abort) MapReduce的计算。

### Semantics in the Presence of Failures

这部分较难理解

## Locality

GFS divides each file into 64 MB blocks, and stores several copies of each block (typically 3 copies) on different machines。

MapReduce master节点考虑输入文件的位置信息，并尝试在包含对应输入数据副本的机器上调度Map任务。如果做不到这一点，它将尝试在任务输入数据的副本附近调度一个Map任务(例如，在与包含数据的机器在同一台网络交换机上的工作机器上)。大多数据从本地读取，不消耗带宽.

## Task Granularity

讲M和R的取值。

理想状态下，M和R应该大于worker machine。这样不仅可以提高动态负载均衡的性能，当worker fail时也可以加速恢复（the many map tasks it has completed can be spread out across all the other worker machines. **没看懂是什么原因可以加速？**）

the master must make O(M + R) scheduling decisions and keeps O(M ∗ R) state in memory as described above。（为什么需要维护这么多状态）平均每个状态的维护（map/reduce task）一个字节的大小，开销不大。

我们选取的M使得可以将数据分割成在16~64MB大小的块，而R的大小通常是worker machine数目的小几倍。

## Backup Tasks

造成MapReduce操作花费较长时间的因素是“straggler”的存在。

- ”straggler“：一台花费较长时间来完成最后几个map或者reduce任务的机器

例如存在可修复问题的磁盘的读取速度会很慢，master将任务分配给它将会需要很长的时间才能完成，或者缓存出现问题的机器，直接读写磁盘的IO速度也很慢。

解决方案：

- 当MapReduce操作将要完成时，master备份in-progress 任务的执行情况。The task is marked as completed whenever either the primary or the backup execution completes.



## Refinements

### Partitioning Function

我们使用 partitioning Function 将中间键值对根据键切分成R个region，默认的切分函数使用` hash(key) mod R`，会生成相当平衡的分区。

我们可以使用自定义的分区函数，比如将所有URL按照用户进行分组，如`hast(Hostname(URL)) mod R`

### Ordering Guarantees

无论是否需要，排序是默认的，在一个给定的分区中，中间键/值对按照键的递增顺序进行处理。

### Combiner Function

We allow the user to specify an **optional** Combiner function that does **partial merging** of this data before it is sent over the network.The Combiner function is executed on **each machine that performs a map task**.

The only difference between a reduce function and a combiner function is how the MapReduce library handles the output of the function. The output of a reduce function is written to the final output file. The output of a combiner function is written to an intermediate file that will be sent to a reduce task.

Partial combining significantly **speeds up certain classes of MapReduce operations.**

### Input and Output Types.

The MapReduce library provides support for reading input data in several different formats.

1. 文本模式的输入将每行视为键值对，其中key为文本内容的偏移，value是具体内容
2. 其它普遍支持的格式存储，按key排序的键值对序列

这样每种格式都可以为map分割成许多片。用户可以通过`reader`接口添加对新类型的支持，例如可以定义如何从数据库读取数据的`reader`

### Skipping Bad Records

用户的代码可能会在输入的某些数据记录的运算出现错误，通常我们会修复bug，但可能使用的代码不能访问；

有时候忽略某些数据记录也是可以接受的

- 在调用map、reduce operation之前，MapReduce运行时库会在全局变量上存储参数的序列号，当用户代码触发信号时，信号处理函数发送包含参数序列号的"last gasp"UDP包给Master节点，master接受到多个对于该数据记录的错误时，会在下一次重新执行相关Map或Reduce任务时跳过该数据记录。

### Local Execution

在分布式系统上的debug相关。

实际的计算是在分布式系统中进行的，通常是在数千台机器上，而工作分配决策是由主服务器动态做出的。为了方便调试、分析和小规模测试，我们开发了一个MapReduce库的替代实现，它在本地机器上依次执行MapReduce操作的所有工作。控件被提供给用户，以便将计算限制在特定的Map任务中，使得用户可以使用像gdb等调试工具。



###  Status Information

The master runs an internal HTTP server and exports a set of status pages for human consumption.

执行状态等信息可通过网页浏览。







# 分布式系统

## 拜占庭将军问题



## Raft

- 一致性算法：保证在分布式系统中每个节点都顺序执行相同的操作序列，在分布式系统的每个节点中执行相同的一致性算法能够保证数据的一致性．

### In Search of an Understandable Consensus Algorithm

#### Section 1  Introduction

　　Raft is a consensus algorithm for **managing a replicated log.** 

(简单讲：Raft一致性算法保证replicated log的一致性，使得服务器集群使用相同replicated state machines即状态机加载log得到相同的输出)

​        Paxos is quite difficult to understand. In order to **enhance understandability**, Raft separates the key elements of consensus, such as **leader election**, **log replication**, and **safety**, and it enforcesa stronger degree of coherency to reduce the number of states that must be considered.

(简单讲：Paxos太难啃 *(Section 3描述有多麻烦)*，重新开发个容易理解的一致性算法，强调understandability)

​        Raft is similar in many ways to existing consensus algorithm( most notably, ***Oki and Liskov's Viewstamped Replication***), but it has several novel features:

- **Strong leader:** For example , log entries only flow from leader to other servers.
- **Leader  election**: Raft uses randomized timers to elect leaders. 
- **Membership changes**: allows the cluster to continue operating normally during configuration changes.

#### Section 2  Replicated state machines

​     Replicated state machines are used to **solve a variety of fault to tolerance problems** in distributed systems.  For example, large-scale systems that have a single cluster leader, such as *GFS*,*HDFS* and *RAMCloud*, typically use a separate replicates state machines to manage leader election and store configuration information that must survice leader crashes.  Examples of replicated state machines include Chubby and ZooKeeper.

​     Consensus algorithms for practical systems typically have the following properites:

1. They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, deplication, and reordering.

   （简单讲：一致性算法确保在**非拜占庭条件**时安全）

2. They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; They may later recover from state on stable storage and rejoin the cluster.

   （简单讲：五个服务器的集群可以容忍任意两个服务器失败，并且他们可以重新恢复状态并加入集群）

3. They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems.

   （简单讲：不依赖计时来保证*logs*的一致性）

   4. In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.

      （简单讲：通常情况下，集群的大多数服务器回回应远程系统调用，小部分较慢的服务器不会影响系统的综合表现）

## Paxos

## ViewStamp



##### 实现MIT Lab

