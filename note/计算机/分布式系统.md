







# 分布式理论

## CAP理论

- **一致性(Consistency)**：所有节点在同一时间的数据完全一致

  - `all nodes see the same data at the same time`

  - 对于一致性，可以分为从客户端和服务端两个不同的视角。

    - 从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。
    - 从服务端来看，则是更新如何分布到整个系统，以保证数据最终一致。

  - 对于一致性，可以分为强/弱/最终一致性三类

    - 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。

    - 弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。

    - 最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。

- **可用性(Availabiliay)**：服务在正常响应时间内一直可用

  - `Reads and writes always succeed`

- **分区容错性(Partition Tolerance)**：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。

  - `the system continues to operate despite arbitrary message loss or failure of part of the system`

### CAP的简单证明：

假设有两个节点 n1 和 n2 之间通过网络相连，每个节点内对应的持久化数据为 d1 和 d2．

n1 节点接受客户请求，更新数据d1，而此时 n1 和 n2之间的网络断开，n1无法将该更新通知给 n2，此时在一致性和可用性只能二选一：

1. 若选择可用性，即 n2节点应该响应旧数据给用户，不能保证一致性
2. 若选择一致性，即 n2节点等待网络恢复之后进行数据一致同步，再响应新数据给用户，此时响应取决与网络何时恢复，不能保证可用性．

### CAP的权衡

#### CAP是二选一的问题而不是三选二的问题

首先应该明确，分区容错性是必须保证的，若没有分区容错性，一旦单机崩溃就没有一致性和可用性之谈．

#### CAP的权衡没有定论，只能根据适用场景

- **CP without A**：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。对于涉及到钱财这样不能有一丝让步的场景，C必须保证。
- **AP wihtout C**：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。

### 举例

以注册中心来探讨一下 CAP 的实际应用。

- 什么是注册中心？

  注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。**节点向注册中心注册服务，用户订阅．**

  <img src="https://pic4.zhimg.com/v2-b0608ad8770af0a37cc0649d99ab13d7_r.jpg" style="zoom:100%;" />

  1. **ZooKeeper 保证的是 CP。** 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。
  2. **Eureka 保证的则是 AP。** Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么  Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper  那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka  保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。
  3. **Nacos 不仅支持 CP 也支持 AP。**



## BASE理论

BASE 理论是**对 CAP 中一致性 C 和可用性 A 权衡的结果**(AP方案的一个补充)，**即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。**其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。

- **Basically Available（基本可用）**：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这**绝不等价于系统不可用。**
  - 什么叫允许损失部分可用性：
    - **响应时间上的损失**: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
    - **系统功能上的损失**：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。
- **Soft-state（软状态）** ：允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- **Eventually Consistent（最终一致性）**:最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。即延迟`at the same time`

## 总结

**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**





# 分布式系统

## 拜占庭将军问题



## Raft

- 一致性算法：保证在分布式系统中每个节点都顺序执行相同的操作序列，在分布式系统的每个节点中执行相同的一致性算法能够保证数据的一致性．

### In Search of an Understandable Consensus Algorithm

#### Section 1  Introduction

　　Raft is a consensus algorithm for **managing a replicated log.** 

(简单讲：Raft一致性算法保证replicated log的一致性，使得服务器集群使用相同replicated state machines即状态机加载log得到相同的输出)

​        Paxos is quite difficult to understand. In order to **enhance understandability**, Raft separates the key elements of consensus, such as **leader election**, **log replication**, and **safety**, and it enforcesa stronger degree of coherency to reduce the number of states that must be considered.

(简单讲：Paxos太难啃 *(Section 3描述有多麻烦)*，重新开发个容易理解的一致性算法，强调understandability)

​        Raft is similar in many ways to existing consensus algorithm( most notably, ***Oki and Liskov's Viewstamped Replication***), but it has several novel features:

- **Strong leader:** For example , log entries only flow from leader to other servers.
- **Leader  election**: Raft uses randomized timers to elect leaders. 
- **Membership changes**: allows the cluster to continue operating normally during configuration changes.

#### Section 2  Replicated state machines

​     Replicated state machines are used to **solve a variety of fault to tolerance problems** in distributed systems.  For example, large-scale systems that have a single cluster leader, such as *GFS*,*HDFS* and *RAMCloud*, typically use a separate replicates state machines to manage leader election and store configuration information that must survice leader crashes.  Examples of replicated state machines include Chubby and ZooKeeper.

​     Consensus algorithms for practical systems typically have the following properites:

1. They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, deplication, and reordering.

   （简单讲：一致性算法确保在**非拜占庭条件**时安全）

2. They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; They may later recover from state on stable storage and rejoin the cluster.

   （简单讲：五个服务器的集群可以容忍任意两个服务器失败，并且他们可以重新恢复状态并加入集群）

3. They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems.

   （简单讲：不依赖计时来保证*logs*的一致性）

   4. In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.

      （简单讲：通常情况下，集群的大多数服务器回回应远程系统调用，小部分较慢的服务器不会影响系统的综合表现）

## Paxos

## ViewStamp



##### 实现MIT Lab

