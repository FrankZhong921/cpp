







# 分布式理论

## CAP理论

- **一致性(Consistency)**：所有节点在同一时间的数据完全一致

  - `all nodes see the same data at the same time`

  - 对于一致性，可以分为从客户端和服务端两个不同的视角。

    - 从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。
    - 从服务端来看，则是更新如何分布到整个系统，以保证数据最终一致。

  - 对于一致性，可以分为强/弱/最终一致性三类

    - 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。

    - 弱一致性：如果能容忍后续的部分或者全部访问不到，则是弱一致性。

    - 最终一致性：如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。

- **可用性(Availabiliay)**：服务在正常响应时间内一直可用

  - `Reads and writes always succeed`

- **分区容错性(Partition Tolerance)**：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。

  - `the system continues to operate despite arbitrary message loss or failure of part of the system`

### CAP的简单证明：

假设有两个节点 n1 和 n2 之间通过网络相连，每个节点内对应的持久化数据为 d1 和 d2．

n1 节点接受客户请求，更新数据d1，而此时 n1 和 n2之间的网络断开，n1无法将该更新通知给 n2，此时在一致性和可用性只能二选一：

1. 若选择可用性，即 n2节点应该响应旧数据给用户，不能保证一致性
2. 若选择一致性，即 n2节点等待网络恢复之后进行数据一致同步，再响应新数据给用户，此时响应取决与网络何时恢复，不能保证可用性．

### CAP的权衡

#### CAP是二选一的问题而不是三选二的问题

首先应该明确，分区容错性是必须保证的，若没有分区容错性，一旦单机崩溃就没有一致性和可用性之谈．

#### CAP的权衡没有定论，只能根据适用场景

- **CP without A**：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。对于涉及到钱财这样不能有一丝让步的场景，C必须保证。
- **AP wihtout C**：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。

### 举例

以注册中心来探讨一下 CAP 的实际应用。

- 什么是注册中心？

  注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。**节点向注册中心注册服务，用户订阅．**

  <img src="https://pic4.zhimg.com/v2-b0608ad8770af0a37cc0649d99ab13d7_r.jpg" style="zoom:100%;" />

  1. **ZooKeeper 保证的是 CP。** 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。
  2. **Eureka 保证的则是 AP。** Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么  Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper  那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka  保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。
  3. **Nacos 不仅支持 CP 也支持 AP。**



## BASE理论

BASE 理论是**对 CAP 中一致性 C 和可用性 A 权衡的结果**(AP方案的一个补充)，**即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。**其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。

- **Basically Available（基本可用）**：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这**绝不等价于系统不可用。**
  - 什么叫允许损失部分可用性：
    - **响应时间上的损失**: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
    - **系统功能上的损失**：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。
- **Soft-state（软状态）** ：允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- **Eventually Consistent（最终一致性）**:最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。即延迟`at the same time`

## 总结

**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**



# 论文阅读

# MapReduce

## Programming Model

The computation takes a set of input key/value pairs, and produces a set of output key/value pairs. The user of the MapReduce library expresses the computation as two functions: Map and Reduce. 

- Map, written by the user, takes an input pair and produces a set of intermediate key/value pairs. 
- The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function. 
- The Reduce function, also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory.

## Example

Consider the problem of counting the number of occurrences of each word in a large collection of documents. The user would write code similar to the following pseudo-code:

```c++
map(String key, String values):
	// key : document name
	// values : document content
	for each word w in values:
		EmitIntermedia(w,"1");
reduce(String key, Iterator values):
	// key : a word
	// values : a list of counts
	int result = 0;
	for each v in values:
		result += ParseInt(v);
	Emit(AsString(result));
```

In addition, the user writes code to fill in a mapreduce specification object with the names of the input and output files, and optional tuning parameters. The user then invokes the MapReduce function, passing it the specification object. The user’s code is linked together with the MapReduce library (implemented in C++).

### Example implement

```c++
#include "mapreduce/mapreduce.h"
// User’s map function
class WordCounter : public Mapper {
	public:
		virtual void Map(const MapInput& input) {
		const string& text = input.value();
		const int n = text.size();
		for (int i = 0; i < n; ) {
		// Skip past leading whitespace
			while ((i < n) && isspace(text[i]))
				i++;
			// Find word end
			int start = i;
			while ((i < n) && !isspace(text[i]))
				i++;
            if (start < i)
				Emit(text.substr(start,i-start),"1");
		}
	}
};

REGISTER_MAPPER(WordCounter);
// User’s reduce function
class Adder : public Reducer {
		virtual void Reduce(ReduceInput* input) {
		// Iterate over all entries with the
		// same key and add the values
		int64 value = 0;
		while (!input->done()) {
			value += StringToInt(input->value());
			input->NextValue();
		}
		// Emit sum for input->key()
		Emit(IntToString(value));
	}
};
REGISTER_REDUCER(Adder);
int main(int argc, char** argv) {
	ParseCommandLineFlags(argc, argv);
	MapReduceSpecification spec;
	// Store list of input files into "spec"
	for (int i = 1; i < argc; i++) {
	MapReduceInput* input = spec.add_input();
		input->set_format("text");
    	input->set_filepattern(argv[i]);
		input->set_mapper_class("WordCounter");
	}
    // Specify the output files:
    // /gfs/test/freq-00000-of-00100
    // /gfs/test/freq-00001-of-00100
    // ...
    MapReduceOutput* out = spec.output();
    out->set_filebase("/gfs/test/freq");
    out->set_num_tasks(100);
    out->set_format("text");
    out->set_reducer_class("Adder");
    // Optional: do partial sums within map
    // tasks to save network bandwidth
    out->set_combiner_class("Adder");
    // Tuning parameters: use at most 2000
    // machines and 100 MB of memory per task
    spec.set_machines(2000);
    spec.set_map_megabytes(100);
    spec.set_reduce_megabytes(100);
    // Now run it
    MapReduceResult result;
    if (!MapReduce(spec, &result)) abort();
    // Done: ’result’ structure contains info
    // about counters, time taken, number of
    // machines used, etc.
    return 0;
}
```

## Implement

<img src="E:\cpp\note\note_picture\image-20210607001425646.png" alt="image-20210607001425646" style="zoom:80%;" />

Many different implementations of the MapReduce interface are possible. The right choice depends on the environment. For example, one implementation may be suitable for a small shared-memory machine, another for a large NUMA multi-processor, and yet another for an even larger collection of networked machines.

- MapReduce的接口实现应该依赖于环境，有的适合在小型共享内存的机器运行，有的适合大型NUMA多处理器，还有的适合更大的联网机器集群。
- **非统一内存访问（NUMA）**是一种用于多处理器的电脑内存体设计，内存访问时间取决于处理器的内存位置。 在NUMA下，处理器访问它自己的本地存储器的速度比非本地存储器（存储器的地方到另一个处理器之间共享的处理器或存储器）快一些。
  - NUMA通过提供分离的存储器给各个处理器，避免当多个处理器访问同一个存储器产生的性能损失来试图解决这个问题。对于涉及到分散的数据的应用（在服务器和类似于服务器的应用中很常见），NUMA可以通过一个共享的存储器提高性能至n倍,而n大约是处理器（或者分离的存储器）的个数。

# 分布式系统

## 拜占庭将军问题



## Raft

- 一致性算法：保证在分布式系统中每个节点都顺序执行相同的操作序列，在分布式系统的每个节点中执行相同的一致性算法能够保证数据的一致性．

### In Search of an Understandable Consensus Algorithm

#### Section 1  Introduction

　　Raft is a consensus algorithm for **managing a replicated log.** 

(简单讲：Raft一致性算法保证replicated log的一致性，使得服务器集群使用相同replicated state machines即状态机加载log得到相同的输出)

​        Paxos is quite difficult to understand. In order to **enhance understandability**, Raft separates the key elements of consensus, such as **leader election**, **log replication**, and **safety**, and it enforcesa stronger degree of coherency to reduce the number of states that must be considered.

(简单讲：Paxos太难啃 *(Section 3描述有多麻烦)*，重新开发个容易理解的一致性算法，强调understandability)

​        Raft is similar in many ways to existing consensus algorithm( most notably, ***Oki and Liskov's Viewstamped Replication***), but it has several novel features:

- **Strong leader:** For example , log entries only flow from leader to other servers.
- **Leader  election**: Raft uses randomized timers to elect leaders. 
- **Membership changes**: allows the cluster to continue operating normally during configuration changes.

#### Section 2  Replicated state machines

​     Replicated state machines are used to **solve a variety of fault to tolerance problems** in distributed systems.  For example, large-scale systems that have a single cluster leader, such as *GFS*,*HDFS* and *RAMCloud*, typically use a separate replicates state machines to manage leader election and store configuration information that must survice leader crashes.  Examples of replicated state machines include Chubby and ZooKeeper.

​     Consensus algorithms for practical systems typically have the following properites:

1. They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, deplication, and reordering.

   （简单讲：一致性算法确保在**非拜占庭条件**时安全）

2. They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; They may later recover from state on stable storage and rejoin the cluster.

   （简单讲：五个服务器的集群可以容忍任意两个服务器失败，并且他们可以重新恢复状态并加入集群）

3. They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems.

   （简单讲：不依赖计时来保证*logs*的一致性）

   4. In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.

      （简单讲：通常情况下，集群的大多数服务器回回应远程系统调用，小部分较慢的服务器不会影响系统的综合表现）

## Paxos

## ViewStamp



##### 实现MIT Lab

